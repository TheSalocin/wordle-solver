{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "dir2 = os.path.abspath('')\n",
    "dir1 = os.path.dirname(dir2)\n",
    "if not dir1 in sys.path: \n",
    "    sys.path.append(dir1)\n",
    "\n",
    "from game.wordle import Wordle\n",
    "from game.util import read_to_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordlist = read_to_lines(\"../data/de_wiktionary_5_letter.txt\")\n",
    "\n",
    "de_config = {\n",
    "    'max_guesses': str(6),\n",
    "\t# The set of words that can potentially be solutions\n",
    "\t'candidate_set': wordlist,\n",
    "\t# The set of words that can be guessed validly\n",
    "\t'guess_set': wordlist\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_list(file):\n",
    "    result = []\n",
    "    with open(file) as fp:\n",
    "        result.extend([word.strip() for word in fp.readlines()])\n",
    "    return result\n",
    "\n",
    "foo = get_word_list(\"../data/de_wiktionary_5_letter.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# have: Wordle class, 3b1b util functions\n",
    "# want: solver base. Solver talks to Wordle giving a guess, then takes answer from Wordle to determine next guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from game.util import get_n_from_word_set\n",
    "from game.constants import DEFAULT_GAME_CONFIG\n",
    "from typing import Dict, List, Tuple, Set\n",
    "import random\n",
    "\n",
    "class Solver:\n",
    "    \n",
    "    def __init__(self, config: Dict[str, str] = DEFAULT_GAME_CONFIG, manual = False, verbose=True):\n",
    "        \n",
    "        if not 'candidate_set' in config or not len(config['candidate_set']): \n",
    "            raise Exception('candidate_set not specified in config')\n",
    "        self.candidate_set = [w.lower() for w in set(config['candidate_set'])]\n",
    "        \n",
    "        if not 'guess_set' in config or not len(config['guess_set']):\n",
    "            self.guess_set = [w.lower() for w in set(config['candidate_set'])]\n",
    "        else:\n",
    "            self.guess_set = [w.lower() for w in set(config['guess_set'])]\n",
    "        self.N = get_n_from_word_set(config['candidate_set'])\n",
    "        self.MAX_GUESSES = int(config['max_guesses'])\n",
    "        self.guess_number = 0\n",
    "        self.guesses = []\n",
    "        self.clues = []\n",
    "        self.states = []\n",
    "        self.verbose = verbose\n",
    "        self.manual = manual\n",
    "\n",
    "    def choose_word(self):\n",
    "        # random guesses for now to test\n",
    "        if self.guess_number >= self.MAX_GUESSES:\n",
    "            if self.verbose:\n",
    "                print(\"Max guesses reached!\")\n",
    "            return None\n",
    "        \n",
    "        guess = random.choice(self.candidate_set)\n",
    "        self.guess_number += 1\n",
    "        self.guesses.append(guess)\n",
    "\n",
    "        return guess\n",
    "    \n",
    "    def incorporate_guess_feedback(self, clue: list, state: int):\n",
    "        self.clues.append(clue)\n",
    "        self.states.append(state)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERLOS\n",
      "ðŸŸ¨â¬›ðŸŸ¨â¬›ðŸŸ¨\n",
      "KAPPT\n",
      "â¬›â¬›â¬›â¬›â¬›\n",
      "KUTTE\n",
      "â¬›â¬›â¬›â¬›ðŸŸ¨\n",
      "KPDSU\n",
      "â¬›â¬›â¬›ðŸŸ¨â¬›\n",
      "SACKE\n",
      "ðŸŸ¨â¬›â¬›â¬›ðŸŸ¨\n"
     ]
    }
   ],
   "source": [
    "random_solve = Solver(config=de_config)\n",
    "w = Wordle(\"lÃ¶sen\", config=de_config)\n",
    "\n",
    "for i in range(5):\n",
    "    guess = random_solve.choose_word()\n",
    "    clue, state = w.guess(guess)\n",
    "    random_solve.get_guess_feedback(clue, state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools as it\n",
    "import numpy as np\n",
    "\n",
    "class EntropySolver(Solver):\n",
    "    # TODO: for simplicity i'm only implementing this for all candidate words for now;\n",
    "    # 3b1b code also contains code for short list, check where self.guess_set used\n",
    "    # TODO: 3b1b stores results as ternary ints instead of list of ints. Incorporate that into other code\n",
    "    # Note: possible -> guess_set, allowed -> candidate_set\n",
    "\n",
    "    def __init__(self, config: Dict[str, str] = DEFAULT_GAME_CONFIG, manual = False, verbose=True, \n",
    "                CHUNK_LENGTH = 13000):\n",
    "        super().__init__(config, manual, verbose)\n",
    "        self.DATA_DIR = os.path.join(\n",
    "            os.path.dirname(os.path.abspath('')),\n",
    "            \"data\",\n",
    "        )\n",
    "        self.PATTERN_MATRIX_FILE = os.path.join(self.DATA_DIR, \"pattern_matrix.npy\")\n",
    "        self.WORD_FREQ_FILE = os.path.join(DATA_DIR, \"wordle_words_freqs_full.txt\")\n",
    "        self.WORD_FREQ_MAP_FILE = os.path.join(self.DATA_DIR, \"freq_map.json\")\n",
    "        self.PATTERN_GRID_DATA = dict()\n",
    "        self.CHUNK_LENGTH = CHUNK_LENGTH\n",
    "        self.MISS = np.uint8(0)\n",
    "        self.MISPLACED = np.uint8(1)\n",
    "        self.EXACT = np.uint8(2)\n",
    "\n",
    "    def get_uniform_prior(self):\n",
    "        return dict(\n",
    "            (w, 1)\n",
    "            for w in self.guess_set\n",
    "        )\n",
    "    \n",
    "    def get_true_wordle_prior(self):\n",
    "        # TODO: so far guess set and candidate set are equal; make better\n",
    "        return dict(\n",
    "            (w, int(w in self.candidate_set))\n",
    "            for w in self.guess_set\n",
    "        )\n",
    "    \n",
    "    #TODO: improve this with hyperparameter tuning?\n",
    "    def get_frequency_based_priors(n_common=3000, width_under_sigmoid=10):\n",
    "        \"\"\"\n",
    "        We know that that list of wordle answers was curated by some human\n",
    "        based on whether they're sufficiently common. This function aims\n",
    "        to associate each word with the likelihood that it would actually\n",
    "        be selected for the final answer.\n",
    "\n",
    "        Sort the words by frequency, then apply a sigmoid along it.\n",
    "        \"\"\"\n",
    "        freq_map = self.get_word_frequencies()\n",
    "        words = np.array(list(freq_map.keys()))\n",
    "        freqs = np.array([freq_map[w] for w in words])\n",
    "        arg_sort = freqs.argsort()\n",
    "        sorted_words = words[arg_sort]\n",
    "\n",
    "        # We want to imagine taking this sorted list, and putting it on a number\n",
    "        # line so that it's length is 10, situating it so that the n_common most common\n",
    "        # words are positive, then applying a sigmoid\n",
    "        x_width = width_under_sigmoid\n",
    "        c = x_width * (-0.5 + n_common / len(words))\n",
    "        xs = np.linspace(c - x_width / 2, c + x_width / 2, len(words))\n",
    "        priors = dict()\n",
    "        for word, x in zip(sorted_words, xs):\n",
    "            priors[word] = sigmoid(x)\n",
    "        return priors\n",
    "\n",
    "    # TODO: adapt this for leipzig word frequency list\n",
    "    def get_word_frequencies(regenerate=False):\n",
    "        if os.path.exists(self.WORD_FREQ_MAP_FILE) or regenerate:\n",
    "            with open(self.WORD_FREQ_MAP_FILE) as fp:\n",
    "                result = json.load(fp)\n",
    "            return result\n",
    "        # Otherwise, regenerate\n",
    "        freq_map = dict()\n",
    "        with open(self.WORD_FREQ_FILE) as fp:\n",
    "            for line in fp.readlines():\n",
    "                pieces = line.split(' ')\n",
    "                word = pieces[0]\n",
    "                freqs = [\n",
    "                    float(piece.strip())\n",
    "                    for piece in pieces[1:]\n",
    "                ]\n",
    "                freq_map[word] = np.mean(freqs[-5:])\n",
    "        with open(self.WORD_FREQ_MAP_FILE, 'w') as fp:\n",
    "            json.dump(freq_map, fp)\n",
    "        return freq_map\n",
    "\n",
    "    def words_to_int_arrays(self, words):\n",
    "        return np.array([[ord(c)for c in w] for w in words], dtype=np.uint8)\n",
    "\n",
    "    def generate_pattern_matrix(self, words1, words2):\n",
    "        \"\"\"\n",
    "        A pattern for two words represents the wordle-similarity\n",
    "        pattern (grey -> 0, yellow -> 1, green -> 2) but as an integer\n",
    "        between 0 and 3^5. Reading this integer in ternary gives the\n",
    "        associated pattern.\n",
    "\n",
    "        This function computes the pairwise patterns between two lists\n",
    "        of words, returning the result as a grid of hash values. Since\n",
    "        this can be time-consuming, many operations that can be are vectorized\n",
    "        (perhaps at the expense of easier readibility), and the the result\n",
    "        is saved to file so that this only needs to be evaluated once, and\n",
    "        all remaining pattern matching is a lookup.\n",
    "        \"\"\"\n",
    "\n",
    "        # Number of letters/words\n",
    "        nl = len(words1[0])\n",
    "        nw1 = len(words1)  # Number of words\n",
    "        nw2 = len(words2)  # Number of words\n",
    "\n",
    "        # Convert word lists to integer arrays\n",
    "        word_arr1, word_arr2 = map(self.words_to_int_arrays, (words1, words2))\n",
    "\n",
    "        # equality_grid keeps track of all equalities between all pairs\n",
    "        # of letters in words. Specifically, equality_grid[a, b, i, j]\n",
    "        # is true when words[i][a] == words[b][j]\n",
    "        equality_grid = np.zeros((nw1, nw2, nl, nl), dtype=bool)\n",
    "        for i, j in it.product(range(nl), range(nl)):\n",
    "            equality_grid[:, :, i, j] = np.equal.outer(word_arr1[:, i], word_arr2[:, j])\n",
    "\n",
    "        # full_pattern_matrix[a, b] should represent the 5-color pattern\n",
    "        # for guess a and answer b, with 0 -> grey, 1 -> yellow, 2 -> green\n",
    "        full_pattern_matrix = np.zeros((nw1, nw2, nl), dtype=np.uint8)\n",
    "\n",
    "        # Green pass\n",
    "        for i in range(nl):\n",
    "            matches = equality_grid[:, :, i, i].flatten()  # matches[a, b] is true when words[a][i] = words[b][i]\n",
    "            full_pattern_matrix[:, :, i].flat[matches] = self.EXACT\n",
    "\n",
    "            for k in range(nl):\n",
    "                # If it's a match, mark all elements associated with\n",
    "                # that letter, both from the guess and answer, as covered.\n",
    "                # That way, it won't trigger the yellow pass.\n",
    "                equality_grid[:, :, k, i].flat[matches] = False\n",
    "                equality_grid[:, :, i, k].flat[matches] = False\n",
    "\n",
    "        # Yellow pass\n",
    "        for i, j in it.product(range(nl), range(nl)):\n",
    "            matches = equality_grid[:, :, i, j].flatten()\n",
    "            full_pattern_matrix[:, :, i].flat[matches] = self.MISPLACED\n",
    "            for k in range(nl):\n",
    "                # Similar to above, we want to mark this letter\n",
    "                # as taken care of, both for answer and guess\n",
    "                equality_grid[:, :, k, j].flat[matches] = False\n",
    "                equality_grid[:, :, i, k].flat[matches] = False\n",
    "\n",
    "        # Rather than representing a color pattern as a lists of integers,\n",
    "        # store it as a single integer, whose ternary representations corresponds\n",
    "        # to that list of integers.\n",
    "        pattern_matrix = np.dot(\n",
    "            full_pattern_matrix,\n",
    "            (3**np.arange(nl)).astype(np.uint8)\n",
    "        )\n",
    "\n",
    "        return pattern_matrix\n",
    "\n",
    "    def chunks(self, lst, n):\n",
    "        \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "        for i in range(0, len(lst), n):\n",
    "            yield lst[i:i + n]\n",
    "\n",
    "    def generate_pattern_matrix_in_blocks(self, many_words1, many_words2):\n",
    "        block_matrix = None\n",
    "        for words1 in self.chunks(many_words1, self.CHUNK_LENGTH):\n",
    "            row = None\n",
    "\n",
    "            for words2 in self.chunks(many_words2, self.CHUNK_LENGTH):\n",
    "                block = self.generate_pattern_matrix(words1, words2)\n",
    "\n",
    "                if row is None:\n",
    "                    row = block\n",
    "                else:\n",
    "                    row = np.hstack((row, block))\n",
    "\n",
    "            if block_matrix is None:\n",
    "                block_matrix = row\n",
    "            else:\n",
    "                block_matrix = np.vstack((block_matrix, row))\n",
    "\n",
    "        return block_matrix\n",
    "\n",
    "    def generate_full_pattern_matrix(self):\n",
    "        pattern_matrix = self.generate_pattern_matrix_in_blocks(self.guess_set, self.guess_set)\n",
    "        # Save to file\n",
    "        np.save(self.PATTERN_MATRIX_FILE, pattern_matrix)\n",
    "        return pattern_matrix\n",
    "\n",
    "    def get_pattern_matrix(self, words1, words2):\n",
    "        if not self.PATTERN_GRID_DATA:\n",
    "            if not os.path.exists(self.PATTERN_MATRIX_FILE):\n",
    "                log.info(\"\\n\".join([\n",
    "                    \"Generating pattern matrix. This takes a minute, but\",\n",
    "                    \"the result will be saved to file so that it only\",\n",
    "                    \"needs to be computed once.\",\n",
    "                ]))\n",
    "                self.generate_full_pattern_matrix()\n",
    "            self.PATTERN_GRID_DATA['grid'] = np.load(self.PATTERN_MATRIX_FILE)\n",
    "            self.PATTERN_GRID_DATA['words_to_index'] = dict(zip(\n",
    "                self.guess_set, it.count()\n",
    "            ))\n",
    "\n",
    "        full_grid = self.PATTERN_GRID_DATA['grid']\n",
    "        words_to_index = self.PATTERN_GRID_DATA['words_to_index']\n",
    "\n",
    "        indices1 = [words_to_index[w] for w in words1]\n",
    "        indices2 = [words_to_index[w] for w in words2]\n",
    "\n",
    "        return full_grid[np.ix_(indices1, indices2)]\n",
    "\n",
    "    def get_pattern(self, guess, answer):\n",
    "        if self.PATTERN_GRID_DATA:\n",
    "            saved_words = self.PATTERN_GRID_DATA['words_to_index']\n",
    "            if guess in saved_words and answer in saved_words:\n",
    "                return self.get_pattern_matrix([guess], [answer])[0, 0]\n",
    "        return self.generate_pattern_matrix([guess], [answer])[0, 0]\n",
    "\n",
    "    def get_possible_words(self, guess, pattern):\n",
    "        all_patterns = self.get_pattern_matrix([guess], self.guess_set).flatten()\n",
    "        return list(np.array(self.guess_set)[all_patterns == pattern])\n",
    "    \n",
    "    def get_word_buckets(self, guess):\n",
    "        buckets = [[] for x in range(3**5)] #TODO: is this **5 because of word length? If so, replace by self.N\n",
    "        hashes = self.get_pattern_matrix([guess], self.guess_set).flatten()\n",
    "        for index, word in zip(hashes, self.guess_set):\n",
    "            buckets[index].append(word)\n",
    "        return buckets\n",
    "\n",
    "    # Entropy calculation starts here\n",
    "\n",
    "    def get_weights(self, words, priors):\n",
    "        frequencies = np.array([priors[word] for word in words])\n",
    "        total = frequencies.sum()\n",
    "        if total == 0:\n",
    "            return np.zeros(frequencies.shape)\n",
    "        return frequencies / total\n",
    "\n",
    "\n",
    "    def get_pattern_distributions(self, allowed_words, possible_words, weights):\n",
    "        \"\"\"\n",
    "        For each possible guess in allowed_words, this finds the probability\n",
    "        distribution across all of the 3^5 wordle patterns you could see, assuming\n",
    "        the possible answers are in possible_words with associated probabilities\n",
    "        in weights.\n",
    "\n",
    "        It considers the pattern hash grid between the two lists of words, and uses\n",
    "        that to bucket together words from possible_words which would produce\n",
    "        the same pattern, adding together their corresponding probabilities.\n",
    "        \"\"\"\n",
    "        pattern_matrix = self.get_pattern_matrix(allowed_words, possible_words)\n",
    "\n",
    "        n = len(allowed_words)\n",
    "        distributions = np.zeros((n, 3**5))\n",
    "        n_range = np.arange(n)\n",
    "        for j, prob in enumerate(weights):\n",
    "            distributions[n_range, pattern_matrix[:, j]] += prob\n",
    "        return distributions\n",
    "\n",
    "\n",
    "    def entropy_of_distributions(self, distributions, atol=1e-12):\n",
    "        axis = len(distributions.shape) - 1\n",
    "        return entropy(distributions, base=2, axis=axis)\n",
    "\n",
    "\n",
    "    def get_entropies(self, cands, guesses, weights):\n",
    "        if weights.sum() == 0:\n",
    "            return np.zeros(len(cands))\n",
    "        distributions = self.get_pattern_distributions(cands, guesses, weights)\n",
    "        return self.entropy_of_distributions(distributions)\n",
    "\n",
    "\n",
    "    def max_bucket_size(self, guess, possible_words, weights):\n",
    "        dist = self.get_pattern_distributions([guess], possible_words, weights)\n",
    "        return dist.max()\n",
    "\n",
    "\n",
    "    def words_to_max_buckets(self, possible_words, weights):\n",
    "        return dict(\n",
    "            (word, self.max_bucket_size(word, possible_words, weights))\n",
    "            for word in ProgressDisplay(possible_words)\n",
    "        )\n",
    "        # TODO: rudiment? what use in 3b1b\n",
    "        \"\"\"\n",
    "        words_and_maxes = list(w2m.items())\n",
    "        words_and_maxes.sort(key=lambda t: t[1])\n",
    "        words_and_maxes[:-20:-1]\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "    def get_bucket_sizes(self, allowed_words, possible_words):\n",
    "        \"\"\"\n",
    "        Returns a (len(allowed_words), 243) shape array reprenting the size of\n",
    "        word buckets associated with each guess in allowed_words\n",
    "        \"\"\"\n",
    "        weights = np.ones(len(possible_words))\n",
    "        return self.get_pattern_distributions(allowed_words, possible_words, weights)\n",
    "\n",
    "\n",
    "    def get_bucket_counts(self, allowed_words, possible_words):\n",
    "        \"\"\"\n",
    "        Returns the number of separate buckets that each guess in allowed_words\n",
    "        would separate possible_words into\n",
    "        \"\"\"\n",
    "        bucket_sizes = self.get_bucket_sizes(allowed_words, possible_words)\n",
    "        return (bucket_sizes > 0).sum(1)\n",
    "        \n",
    "    # Solvers\n",
    "    def get_guess_values_array(self, priors, look_two_ahead=False):\n",
    "        weights = self.get_weights(self.guess_set, priors)\n",
    "        ents1 = self.get_entropies(self.candidate_set, self.guess_set, weights)\n",
    "        probs = np.array([\n",
    "            0 if word not in self.guess_set else weights[self.guess_set.index(word)]\n",
    "            for word in self.candidate_set\n",
    "        ])\n",
    "\n",
    "        return np.array([ents1, probs])\n",
    "\n",
    "    def entropy_to_expected_score(self, ent):\n",
    "        \"\"\"\n",
    "        Based on a regression associating entropies with typical scores\n",
    "        from that point forward in simulated games, this function returns\n",
    "        what the expected number of guesses required will be in a game where\n",
    "        there's a given amount of entropy in the remaining possibilities.\n",
    "        \"\"\"\n",
    "        # Assuming you can definitely get it in the next guess,\n",
    "        # this is the expected score\n",
    "        min_score = 2**(-ent) + 2 * (1 - 2**(-ent))\n",
    "\n",
    "        # To account for the likely uncertainty after the next guess,\n",
    "        # and knowing that entropy of 11.5 bits seems to have average\n",
    "        # score of 3.5, we add a line to account\n",
    "        # we add a line which connects (0, 0) to (3.5, 11.5)\n",
    "        return min_score + 1.5 * ent / 11.5\n",
    "\n",
    "\n",
    "    def get_expected_scores(self, allowed_words, possible_words, priors,\n",
    "                            look_two_ahead=False,\n",
    "                            n_top_candidates_for_two_step=25,\n",
    "                            ):\n",
    "        # Currenty entropy of distribution\n",
    "        weights = self.get_weights(possible_words, priors)\n",
    "        H0 = self.entropy_of_distributions(weights)\n",
    "        H1s = self.get_entropies(allowed_words, possible_words, weights)\n",
    "\n",
    "        word_to_weight = dict(zip(possible_words, weights))\n",
    "        probs = np.array([word_to_weight.get(w, 0) for w in allowed_words])\n",
    "        # If this guess is the true answer, score is 1. Otherwise, it's 1 plus\n",
    "        # the expected number of guesses it will take after getting the corresponding\n",
    "        # amount of information.\n",
    "        expected_scores = probs + (1 - probs) * (1 + self.entropy_to_expected_score(H0 - H1s))\n",
    "\n",
    "        if not look_two_ahead:\n",
    "            return expected_scores\n",
    "\n",
    "        # For the top candidates, refine the score by looking two steps out\n",
    "        # This is currently quite slow, and could be optimized to be faster.\n",
    "        # But why?\n",
    "        sorted_indices = np.argsort(expected_scores)\n",
    "        allowed_second_guesses = self.guess_set\n",
    "        expected_scores += 1  # Push up the rest\n",
    "        for i in ProgressDisplay(sorted_indices[:n_top_candidates_for_two_step], leave=False):\n",
    "            guess = allowed_words[i]\n",
    "            H1 = H1s[i]\n",
    "            dist = self.get_pattern_distributions([guess], possible_words, weights)[0]\n",
    "            buckets = self.get_word_buckets(guess, possible_words)\n",
    "            second_guesses = [\n",
    "                optimal_guess(allowed_second_guesses, bucket, priors, look_two_ahead=False)\n",
    "                for bucket in buckets\n",
    "            ]\n",
    "            H2s = [\n",
    "                self.get_entropies([guess2], bucket, get_weights(bucket, priors))[0]\n",
    "                for guess2, bucket in zip(second_guesses, buckets)\n",
    "            ]\n",
    "\n",
    "            prob = word_to_weight.get(guess, 0)\n",
    "            expected_scores[i] = sum((\n",
    "                # 1 times Probability guess1 is correct\n",
    "                1 * prob,\n",
    "                # 2 times probability guess2 is correct\n",
    "                2 * (1 - prob) * sum(\n",
    "                    p * word_to_weight.get(g2, 0)\n",
    "                    for p, g2 in zip(dist, second_guesses)\n",
    "                ),\n",
    "                # 2 plus expected score two steps from now\n",
    "                (1 - prob) * (2 + sum(\n",
    "                    p * (1 - word_to_weight.get(g2, 0)) * entropy_to_expected_score(H0 - H1 - H2)\n",
    "                    for p, g2, H2 in zip(dist, second_guesses, H2s)\n",
    "                ))\n",
    "            ))\n",
    "        return expected_scores\n",
    "\n",
    "\n",
    "    def optimal_guess(self, allowed_words, possible_words, priors,\n",
    "                    look_two_ahead=False,\n",
    "                    optimize_for_uniform_distribution=False,\n",
    "                    purely_maximize_information=False,\n",
    "                    ):\n",
    "        if purely_maximize_information:\n",
    "            if len(possible_words) == 1:\n",
    "                return possible_words[0]\n",
    "            weights = self.get_weights(possible_words, priors)\n",
    "            ents = self.get_entropies(allowed_words, possible_words, weights)\n",
    "            return allowed_words[np.argmax(ents)]\n",
    "\n",
    "        # Just experimenting here...\n",
    "        if optimize_for_uniform_distribution:\n",
    "            expected_scores = self.get_score_lower_bounds(\n",
    "                allowed_words, possible_words\n",
    "            )\n",
    "        else:\n",
    "            expected_scores = self.get_expected_scores(\n",
    "                allowed_words, possible_words, priors,\n",
    "                look_two_ahead=look_two_ahead\n",
    "            )\n",
    "        return allowed_words[np.argmin(expected_scores)]\n",
    "\n",
    "\n",
    "    def brute_force_optimal_guess(self, all_words, possible_words, priors, n_top_picks=10, display_progress=False):\n",
    "        if len(possible_words) == 0:\n",
    "            # Doesn't matter what to return in this case, so just default to first word in list.\n",
    "            return all_words[0]\n",
    "        # For the suggestions with the top expected scores, just\n",
    "        # actually play the game out from this point to see what\n",
    "        # their actual scores are, and minimize.\n",
    "        expected_scores = self.get_score_lower_bounds(all_words, possible_words)\n",
    "        top_choices = [all_words[i] for i in np.argsort(expected_scores)[:n_top_picks]]\n",
    "        true_average_scores = []\n",
    "        if display_progress:\n",
    "            iterable = ProgressDisplay(\n",
    "                top_choices,\n",
    "                desc=f\"Possibilities: {len(possible_words)}\",\n",
    "                leave=False\n",
    "            )\n",
    "        else:\n",
    "            iterable = top_choices\n",
    "\n",
    "        for next_guess in iterable:\n",
    "            scores = []\n",
    "            for answer in possible_words:\n",
    "                score = 1\n",
    "                possibilities = list(possible_words)\n",
    "                guess = next_guess\n",
    "                while guess != answer:\n",
    "                    possibilities = get_possible_words(\n",
    "                        guess, get_pattern(guess, answer),\n",
    "                        possibilities,\n",
    "                    )\n",
    "                    # Make recursive? If so, we'd want to keep track of\n",
    "                    # the next_guess map and pass it down in the recursive\n",
    "                    # subcalls\n",
    "                    guess = optimal_guess(\n",
    "                        all_words, possibilities, priors,\n",
    "                        optimize_for_uniform_distribution=True\n",
    "                    )\n",
    "                    score += 1\n",
    "                scores.append(score)\n",
    "            true_average_scores.append(np.mean(scores))\n",
    "        return top_choices[np.argmin(true_average_scores)]\n",
    "\n",
    "    # TODO: adapt this for my purposes\n",
    "    def gather_entropy_to_score_data(self, first_guess=\"raine\", priors=None):\n",
    "        words = self.guess_set\n",
    "        answers = self.candidate_set\n",
    "        if priors is None:\n",
    "            priors = self.get_true_wordle_prior()\n",
    "\n",
    "        # List of entropy/score pairs\n",
    "        ent_score_pairs = []\n",
    "\n",
    "        for answer in ProgressDisplay(answers):\n",
    "            score = 1\n",
    "            possibilities = list(filter(lambda w: priors[w] > 0, words))\n",
    "            guess = first_guess\n",
    "            guesses = []\n",
    "            entropies = []\n",
    "            while True:\n",
    "                guesses.append(guess)\n",
    "                weights = self.get_weights(possibilities, priors)\n",
    "                entropies.append(self.entropy_of_distributions(weights))\n",
    "                if guess == answer:\n",
    "                    break\n",
    "                possibilities = self.get_possible_words(\n",
    "                    guess, self.get_pattern(guess, answer), possibilities\n",
    "                )\n",
    "                guess = self.optimal_guess(words, possibilities, priors)\n",
    "                score += 1\n",
    "\n",
    "            for sc, ent in zip(it.count(1), reversed(entropies)):\n",
    "                ent_score_pairs.append((ent, sc))\n",
    "\n",
    "        with open(ENT_SCORE_PAIRS_FILE, 'w') as fp:\n",
    "            json.dump(ent_score_pairs, fp)\n",
    "\n",
    "        return ent_score_pairs\n",
    "\n",
    "\n",
    "    def simulate_games(first_guess=None,\n",
    "                    priors=None,\n",
    "                    look_two_ahead=False,\n",
    "                    optimize_for_uniform_distribution=False,\n",
    "                    second_guess_map=None,\n",
    "                    exclude_seen_words=False,\n",
    "                    test_set=None,\n",
    "                    shuffle=False,\n",
    "                    hard_mode=False,\n",
    "                    purely_maximize_information=False,\n",
    "                    brute_force_optimize=False,\n",
    "                    brute_force_depth=10,\n",
    "                    results_file=None,\n",
    "                    next_guess_map_file=None,\n",
    "                    quiet=False,\n",
    "                    ):\n",
    "        all_words = self.guess_set\n",
    "        short_word_list = self.candidate_set\n",
    "\n",
    "        if first_guess is None:\n",
    "            first_guess = self.optimal_guess(\n",
    "                all_words, all_words, priors,\n",
    "                **choice_config\n",
    "            )\n",
    "\n",
    "        if priors is None:\n",
    "            priors = self.get_frequency_based_priors()\n",
    "\n",
    "        if test_set is None:\n",
    "            test_set = short_word_list\n",
    "\n",
    "        if shuffle:\n",
    "            random.shuffle(test_set)\n",
    "\n",
    "        seen = set()\n",
    "\n",
    "        # Function for choosing the next guess, with a dict to cache\n",
    "        # and reuse results that are seen multiple times in the sim\n",
    "        next_guess_map = {}\n",
    "\n",
    "        def get_next_guess(guesses, patterns, possibilities):\n",
    "            phash = \"\".join(\n",
    "                str(g) + \"\".join(map(str, pattern_to_int_list(p)))\n",
    "                for g, p in zip(guesses, patterns)\n",
    "            )\n",
    "            if second_guess_map is not None and len(patterns) == 1:\n",
    "                next_guess_map[phash] = second_guess_map[patterns[0]]\n",
    "            if phash not in next_guess_map:\n",
    "                choices = all_words\n",
    "                if hard_mode:\n",
    "                    for guess, pattern in zip(guesses, patterns):\n",
    "                        choices = get_possible_words(guess, pattern, choices)\n",
    "                if brute_force_optimize:\n",
    "                    next_guess_map[phash] = brute_force_optimal_guess(\n",
    "                        choices, possibilities, priors,\n",
    "                        n_top_picks=brute_force_depth,\n",
    "                    )\n",
    "                else:\n",
    "                    next_guess_map[phash] = optimal_guess(\n",
    "                        choices, possibilities, priors,\n",
    "                        look_two_ahead=look_two_ahead,\n",
    "                        purely_maximize_information=purely_maximize_information,\n",
    "                        optimize_for_uniform_distribution=optimize_for_uniform_distribution,\n",
    "                    )\n",
    "            return next_guess_map[phash]\n",
    "\n",
    "        # Go through each answer in the test set, play the game,\n",
    "        # and keep track of the stats.\n",
    "        scores = np.zeros(0, dtype=int)\n",
    "        game_results = []\n",
    "        for answer in ProgressDisplay(test_set, leave=False, desc=\" Trying all wordle answers\"):\n",
    "            guesses = []\n",
    "            patterns = []\n",
    "            possibility_counts = []\n",
    "            possibilities = list(filter(lambda w: priors[w] > 0, all_words))\n",
    "\n",
    "            if exclude_seen_words:\n",
    "                possibilities = list(filter(lambda w: w not in seen, possibilities))\n",
    "\n",
    "            score = 1\n",
    "            guess = first_guess\n",
    "            while guess != answer:\n",
    "                pattern = get_pattern(guess, answer)\n",
    "                guesses.append(guess)\n",
    "                patterns.append(pattern)\n",
    "                possibilities = get_possible_words(guess, pattern, possibilities)\n",
    "                possibility_counts.append(len(possibilities))\n",
    "                score += 1\n",
    "                guess = get_next_guess(guesses, patterns, possibilities)\n",
    "\n",
    "            # Accumulate stats\n",
    "            scores = np.append(scores, [score])\n",
    "            score_dist = [\n",
    "                int((scores == i).sum())\n",
    "                for i in range(1, scores.max() + 1)\n",
    "            ]\n",
    "            total_guesses = scores.sum()\n",
    "            average = scores.mean()\n",
    "            seen.add(answer)\n",
    "\n",
    "            game_results.append(dict(\n",
    "                score=int(score),\n",
    "                answer=answer,\n",
    "                guesses=guesses,\n",
    "                patterns=list(map(int, patterns)),\n",
    "                reductions=possibility_counts,\n",
    "            ))\n",
    "            # Print outcome\n",
    "            if not quiet:\n",
    "                message = \"\\n\".join([\n",
    "                    \"\",\n",
    "                    f\"Score: {score}\",\n",
    "                    f\"Answer: {answer}\",\n",
    "                    f\"Guesses: {guesses}\",\n",
    "                    f\"Reductions: {possibility_counts}\",\n",
    "                    *patterns_to_string((*patterns, 3**5 - 1)).split(\"\\n\"),\n",
    "                    *\" \" * (6 - len(patterns)),\n",
    "                    f\"Distribution: {score_dist}\",\n",
    "                    f\"Total guesses: {total_guesses}\",\n",
    "                    f\"Average: {average}\",\n",
    "                    *\" \" * 2,\n",
    "                ])\n",
    "                if answer is not test_set[0]:\n",
    "                    # Move cursor back up to the top of the message\n",
    "                    n = len(message.split(\"\\n\")) + 1\n",
    "                    print((\"\\033[F\\033[K\") * n)\n",
    "                else:\n",
    "                    print(\"\\r\\033[K\\n\")\n",
    "                print(message)\n",
    "\n",
    "        final_result = dict(\n",
    "            score_distribution=score_dist,\n",
    "            total_guesses=int(total_guesses),\n",
    "            average_score=float(scores.mean()),\n",
    "            game_results=game_results,\n",
    "        )\n",
    "\n",
    "        # Save results\n",
    "        for obj, file in [(final_result, results_file), (next_guess_map, next_guess_map_file)]:\n",
    "            if file:\n",
    "                path = os.path.join(DATA_DIR, \"simulation_results\", file)\n",
    "                with open(path, 'w') as fp:\n",
    "                    json.dump(obj, fp)\n",
    "\n",
    "        return final_result, next_guess_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = EntropySolver(config=de_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[242,   0,   4, ..., 165,  18,  18],\n",
       "       [  0, 242,  27, ...,  84, 165, 162],\n",
       "       [ 10,   3, 242, ...,   1,   0,   0],\n",
       "       ...,\n",
       "       [189,  10,  27, ..., 242,  10,   1],\n",
       "       [ 18, 163,   0, ...,  82, 242, 180],\n",
       "       [ 18, 162,   0, ...,  81, 180, 242]],\n",
       "      shape=(9314, 9314), dtype=uint8)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.generate_full_pattern_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Å“'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(339)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 1]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_solve.get_guess_feedback(clue, state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0, 0, 0, 1]]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_solve.clues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
